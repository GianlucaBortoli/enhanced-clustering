This work consists in solving the two main problems of "k-means"-like 
algorithms described in Section \ref{related} and \ref{problem_definition}:
choosing the $k$ parameter and placing the $k$ initial centroids.
The only assumption is that the dataset is generated from one or more gaussian
disitributions.


\subsection*{The choice of $k$}
The most common scenario does not involve a domain expert. In this way there is 
no prior knowledge that can be used to guess a proper and reasonable value for $k$.
The main idea behind the solution proposed in this work is that of exploiting density
analysis to get an initial estimate of the number of clusters in the data.

More formally, a \emph{peak detection} procedure is used to retrieve all the local
maxima in the density function for each feature\footnote{This is equivalent to finding 
all the local maxima in the density functions for each axis, if the item considered is
a point in an n-dimensional euclidean space.} of the items in the dataset. In this way,
a peak is defined as a data sample that is either larger than its two neighboring samples.
This results in a list of peaks for each feature space and finally a n-dimensional grid
matching every possible peack among all the features available is created.
In this way, the worst-case scenario is taken into account, allowing this approach not
to miss any potential centroid in the whole dataset\footnote{This approach produces an 
over-estimation of the real number of centroids, including also possible false-negatives.}.
% TODO: inserisci figura che fa vedere sia come si crea la griglia di picchi e della
%       funzione di densità su tutti gli assi
% TODO: pseudocodice di come creiamo questa griglia
Moreover, among all the potential peaks created, only those which have at least one data
item assigned after a run of the k-means are kept. This allows to discard all the 
false-positives that were found creating all the possible touples in all the feature spaces.
Furthermore, it represents the refinement step for the algorithm.

Finally, the number of items in this grid represents the value for the $k$ parameter.


\subsection*{Centroids bootstrap}
The other important phase is to find a good positioning of the $k$ initial centroids.
The main idea behind this feature proceeds as follows:
\begin{enumerate}
    \item \label{step1} use the peak location as the one of the centroids.
    \item build an ellipse around every centroid with the $(a,b)$ parameters (namely the x-axis
        and the y-axis radius) applying the formula in Equation \ref{ellipse_params}.
    \item merge all the ellipses that intersect with each other.
    \item go to \ref{step1} until convergence or some other exit criterion is met.
\end{enumerate}

% TODO: pseudocodice delle procedure di creazione dell'ellisse e merging di ellissi
% TODO: immagini che fa vedere uno step di merge che è andato bene e dimostra la nostra idea

The formula (Equation \ref{ellipse_params}) to compute the heigth and the width of the ellipsis 
is the key aspect of the proposed merging strategy.
\begin{equation}
\label{ellipse_params}
    f(std, cdens) = ((std * 2 * 0.35) + (cdens * 0.7)) * 5
\end{equation}
Where:
\begin{itemize}
    \item $std$ is the standard deviation of the gaussian distribution underlying the cluster
    \item $cdens$ is the value of the gaussian distribution Probability Distribution Function (PDF)
        in the mean.
\end{itemize}

The reasoning behing the formula works as follows. Given the mean and the standard deviation ($std$)
of a cluster, $std$ is doubled to take both the sides of the gaussian distribution into account.
The 0.35 and the 0.7 values allows the density in the mean value of the gaussian distribution ($cdens$) 
to influence more the size of the ellipse rather than its standard deviation ($std$). Finally, 5 is
an overall scaling factor useful to allow the comparison of cluster with very different densities.

From an higher point ov view, Equation \ref{ellipse_params} represents the \emph{estimated influence area} 
for every cluster. As the name suggests, this depicts (also visually) the area of influence a cluster
has on all the others. If two clusters' estimated influence area has a non-empty intersection, this
means that they can be collapsed and become a single bigger cluster.

%TODO: metti qui immagine di due cluster che si fondono per l'area di influenza

A noteworthy aspect is that all the aforementioned factors used inside Equation \ref{ellipse_params} have
been set after several tuning stages through empirical tests using many datasets with different
cluster properties.

Nevertheless, representing the clusters my means of sufficient statistics allows to implement an $O(1)$
merging procedure. More in detail, every cluster is internally represented as a touple with the
following imformation:
\begin{equation*}
    \left(\left[\sum_{p}^{|C|} feat[0],\dots,\sum_{p}^{|C|} feat[n]\right],|C|\right)
\end{equation*}
In an 2-dimensional euclidean space, this metadata result in a list containing the the sum of all the
points' coordinates among the 2 axes and the number of points within the cluster taken into consideration.
This structure contains the least possible information to compute the centroid's coordinates as the
barycenter of that particolar cluster.  

Moreover, this merging strategy enables all the \emph{Data Mining Desiderata} described in Section 
\ref{intro}. It makes use of sufficient statistics to internally represent the clusters, thus 
requiring an overall amount of memory that is linear in the number of peaks ($O(k)$, \emph{limited memory}
property). Furthermore, the \emph{online} behaviour is guaranteed by default, since the centroid
bootstrap and merging procedure always returns a solution at any point in time. Finally, it allows
to deal with \emph{straming} data thanks both to its linear memory consumption and to its ability to
work with chunks of data by default, propagating all the sufficient statistics needed to update the 
centroids from one iteration to another. This also guarantees that if the computation is stopped
it can restart without having to re-process everything from scratch.


\subsection*{Applications}
The solutions presented in this work can have two main applications. On the one hand,
this procedure can be used as a bootstrapping phase for a  "k-means"-like algorithm. 
On the other, it can be integrated with other partitional clustering algorithms to
refine the local solution every certain number of iterations, allowing the system as a
whole to find increasingly better clusters.