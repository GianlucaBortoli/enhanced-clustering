The most popular and simplest partitional algorithm is \textbf{k-means}~\cite{macqueen1967some}.
Like every other solution belonging to this class, it requires the objective number of clusters~($k$) to be known a-priori.
Unfortunately, there exists no mathematical formula to compute such parameter in advance,
requiring the test to be run multiple times with different values in order to find the best solution according to some criterion (\emph{e.g.} the Schwarz Criterion~\cite{schwarz1978estimating}).
This algorithm is based on the notion of distance and it is usually employs the Euclidean one.
The resulting division into clusters can be also seen as a lossy compression of the points towards the centroids identifying the clusters.
The main idea behind the k-means consists in minimizing an objective function.
Usually the Mean Squared Error~(MSE) is chosen, where the error is defined as the distance between each point and the centroid of the cluster it is assigned to.
This process is iterative; initially $k$ points are identified to be the centroids,
then all the points are assigned to the nearest centroid (locally minimazing the MSE)
and finally the centroids are recomputed as the barycenter of the clusters.
The procedure continues until the convergence of the centroids' locations.
A noteworthy aspect is that the bootstrap phase, namely the initial centroids identification, highly influences the outcome.

Different centroids usually lead to different results, since the algorithm is designed to find a local optimum.
Several options for the bootstrap have been proposed like the one from Bradley and Fayyad~\cite{bradley1998refining}.
They suggest to run the k-means algorithm $M$ times using any initial centroid selection strategy
on $M$ different subsets of the initial data.
After that, an optimal grouping of the $M \times k$ centroids identified in the previous runs has to be found.
Given the small set size, a brute force approach is a reasonable option.
Finally the ``real'' k-means will use those centroids as the initial ones.

Using a distance as a similarity measure implies that the clusters will have a spherical shape.
It follows that the algorithm performs best when the input data have features values that are normally distributed.

Despite these disadvantages, many variants and optimizations have been proposed both by the industrial and academic communities~\cite{kanungo2002efficient,likas2003global,elkan2003using}.\\


Another important clustering algorithm is the ``Density-based spatial clustering of applications with noise'', more commonly known as \textbf{DBSCAN}~\cite{ester1996density}.
As the name suggests, it is a density-based approach to the clustering problem,
meaning that it groups together points with many others in the neighborhood
and penalizes the ones in low density areas (outliers).
The original version of DBSCAN relies on two user-provided parameters, namely \emph{minPts} and $\epsilon$.
The \emph{minPts} variable represents the minimum number of points that must lie in a circle of radius $\epsilon$~(neighborhood).

This algorithm exploits the \emph{density-reachability} to define three classes of points:
\begin{itemize}
    \item \emph{core}: set of points that have at least \emph{minPts} neighbors.
    \item \emph{reachable}:
        set of points that are in the neighborhood of a \emph{core} point,
        but are not \emph{core} points themselves.
    \item \emph{outlier}:
        set of points that have less than \emph{minPts} points in their neighborhood.
\end{itemize}

As happens with the k-means, DBSCAN has the disadvantage of requiring its parameters \emph{minPts} and $\epsilon$ to be known in advance.
One possible solution is to let a domain expert deal with it, providing sensible parameters based on his prior and deep knowledge of the dataset.
Moreover, DBSCAN lacks in flexibility since it uses a single ``is dense'' threshold derived from the two input parameters.

Some techniques for estimating such parameters have been proposed in the literature, resulting in an extended version of the algorithm known as EDBSCAN~\cite{elbatta2013dynamic, ram2009enhanced}.
It improves the handling of local density variation that exists within the clusters and dynamically chooses the best \emph{minPts} and $\epsilon$ values for the current run.
For good clustering results, such significant density variation might be allowed within a cluster if the objective is not a large number of smaller unimportant clusters.
Furthermore, it tries to detect the clusters with different shapes and sizes that differ in local density.

As opposed to k-means, DBSCAN is able to find arbitrarly-shaped clusters, since it does not employ a distance to measure similarity.
Moreover, the notion of density-reachability is not symmetric.
Hence, this is the key property that allows to find clusters with any shape rather than only ones normally distributed.\\


A predictable evolution are the mixed approaches, that try to combine the advantages of both the partitional and the hierarchical models to overcome their weaknesses.
One of the major applications to the DBMSs is the one proposed by Bradley, Fayyad and Reina --- \textbf{BFR}~\cite{bradley1998scaling}.
It adresses the problem of clustering very large databases that do not fit in main memory.
Hence, scanning data at each iteration step is extremely costly; this can be generalized to scenarios where random reading operations are costly or not possible (\emph{e.g.} streaming data, hard disk, non materialized views).
The main idea behind BFR is to use sufficent statistics~\cite{fisher1922mathematical} to represent groups of points.

In more detail, it has to be initialized with $k$ points, which will be the intial centroids;
after that, it fetches data filling the preallocated RAM buffer.
At this point it updates the internal model (\emph{i.e.} it runs a classic k-means on the buffered data)
and classifies the singleton items in the following sets to perform data compression:
\begin{itemize}
	\item \emph{discard}: points that can be discarded updating the sufficent statistics.
	\item \emph{compression}: non discarded points that do not belong to any of the $k$ clusters,
		but can be summarized and reppresend by other sufficient statistics.
	\item \emph{retained}: points that cannot be summarized and are kept as-is in the buffer.
\end{itemize}


